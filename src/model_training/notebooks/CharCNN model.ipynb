{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharCNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed theory regarding the `CharCNN` can be found in this [paper](https://arxiv.org/pdf/1509.01626.pdf). The basic idea for a `CharCNN` essentially follow the below process:\n",
    "\n",
    " 1. Obtain text data in raw form\n",
    " 2. Apply quantisation to transform text data into fixed numerical input format\n",
    " 3. Apply 1-D convolution to quantised text data to capture temporal features\n",
    " 4. Learn the weights for the CNN (with 1-D convolution layers) using gradient descent\n",
    "\n",
    "The advantages of the `CharCNN` as claimed by the author in the original paper includes:\n",
    " - CNNs do not require the knowledge of words\n",
    " - CNNs do not require knowledge about the syntactic or semantic structure of a language\n",
    " - The previous point leads to a simplified engineering solution where a single model may be applied to multiple languages\n",
    " - Abnormal character combinations such as misspellings and emoticons may be natrually learnt\n",
    " - Generally CNNs are faster to train when compared to RNNs (typically used for NLP tasks)\n",
    " \n",
    "However, the advantages do come with a prerequisite that the model is trained on a sufficiently large dataset that is reasonably balanced.\n",
    "\n",
    "***Note: The main \"trick\" for a `CharCNN` is the text quantisation preprocessing combined with 1-D convolution (2-D convolutions is not suited in this case as text is sequential and therefore can be considered as one dimensional)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing and text quantisation steps are detailed in the `Data Preparation` notebook. The following sections will detail some of the techinical details for the `CharCNN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CharCNN modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 1-D convolutional module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain 1-D convolution, the simplest approach is by demonstration. Below images shows 1-D convolution with 1-D and 2-D inputs.\n",
    "\n",
    "<center>\n",
    "    <img src=./imgs/conv1d_1din.jpg height=700 width=700>\n",
    "        <figcaption>\n",
    "            <b>1-D convolution with 1-D input</b>\n",
    "        </figcaption>\n",
    "    <img src=./imgs/conv1d_2din.jpg height=700 width=700>\n",
    "        <figcaption>\n",
    "            <b>1-D convolution with 2-D input</b>\n",
    "        </figcaption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle is exactly the same as 2-D convolution, you slide a kernel across the input matrix, apply element-wise multiplication and sum the results to produce a single output element. The only difference here is that we only slide the kernel in the horizontal direction with 1-D convolution.\n",
    "\n",
    "*Note: the output of the 1-D convolution will be a 1-D matrix regardless of input dimension. However, if we want to use many filters (i.e. multi-channel) then the output 1-D matrices will be stacked together to form a 2-D matrix with dimension (1D x N) where N is the number of channels / filters used. This is quite well explained in this StackOverflow [article](https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking our scenario as an example, the input shape will be (70 x 1014) for a single sample (although in reality there can be a sample dimension). This originates from the fact that we have 70 characters in the alphabet and 1014 features specified by the assumed maximum text input length. In this case, the first 1-D convolutional layer in the network should have a filter size of (K x 70) where K is the kernel size. The output of the convolutional layer will then be (C x 1014) where C is the number of filters. Note this assumes that 'SAME' padding is applied and stride = 1.\n",
    "\n",
    "This is demonstrated below in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1014])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix = np.random.rand(1, 70, 1014)\n",
    "input_tensor = Tensor(input_matrix)\n",
    "kernel_size = 7\n",
    "padding = ceil((kernel_size-1)/2) # Assuming default dilation and stride\n",
    "conv = nn.Conv1d(in_channels=70, out_channels=256, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "conv(input_tensor).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: to apply 'SAME' padding, the formula below can be used:\n",
    "\n",
    "<img src=./imgs/conv1d_outshape.png height=700 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 1-D MaxPooling module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Pooling as a concept is very simple to understand. It is used to decrease the size of feature vectors while retaining essential information. The pooling technique exists to allow for the training of deeper neural networks.\n",
    "\n",
    "For a given kernel size and stride, the Max Pool will filter the input matrix by selecting the maximum value within the kernel dimension and return it as a single element output. For non-overlapping max pooling, the stride should be set to the kernel size.\n",
    "\n",
    "Below is a demonstration of 1-D maxplooing using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([1, 2, 9])\n",
      "Output Shape: torch.Size([1, 2, 3])\n",
      "Calculated output feature dimension: 3\n"
     ]
    }
   ],
   "source": [
    "input_matrix = np.random.rand(1, 2, 9)\n",
    "input_tensor = Tensor(input_matrix)\n",
    "kernel_size = 3\n",
    "stride = kernel_size # for non-overlapping max pooling\n",
    "out_shape = ceil((input_tensor.shape[2]-(kernel_size-1)-1)/stride + 1)\n",
    "max_pool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "print(f\"Input Shape: {input_tensor.shape}\")\n",
    "print(f\"Output Shape: {max_pool(input_tensor).shape}\")\n",
    "print(f\"Calculated output feature dimension: {out_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the output feature dimension shape can be calculated using the same formula detailed in the 1-D convolution section. A rule of thumb is that for non-overlapping pooling, the feature dimension will decrease with a factor that is equal to the kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.83814695, 0.07839527, 0.59405836, 0.75269495, 0.34446287,\n",
       "         0.11398736, 0.71846285, 0.63767295, 0.91154655],\n",
       "        [0.39194546, 0.47190193, 0.58148252, 0.28835472, 0.94771464,\n",
       "         0.05098323, 0.67678279, 0.51066291, 0.73233075]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8381, 0.7527, 0.9115],\n",
       "         [0.5815, 0.9477, 0.7323]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pool(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The max pooling for an input with many channels (2-D matrix) will have stacked 1-D output**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Relu as detailed in the paper by Zhang et. al.\n",
    "\n",
    "h(x) = max{0, x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are linear layers usually positioned at the tail end of a neural network, which typically act as a classifier. There are two things to note:\n",
    "\n",
    " - The last fully connected layer will have the same number of units/neurons/outputs as the number of classes existing for prediction.\n",
    " - The first fully connected layer input dimension must correspond to the flattened output dimension of the last convolutional layer.\n",
    " \n",
    "These two points are demonstrated using PyTorch below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of convolution layer: torch.Size([1, 256, 1014])\n",
      "Input features to linear layer: 259584\n",
      "Output shape of the linear layer: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "input_matrix = np.random.rand(1, 70, 1014)\n",
    "input_tensor = Tensor(input_matrix)\n",
    "kernel_size = 7\n",
    "padding = ceil((kernel_size-1)/2) # Assuming default dilation and stride\n",
    "conv = nn.Conv1d(in_channels=70, out_channels=256, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "conv_out = conv(input_tensor)\n",
    "lin_input_features = conv_out.view(conv_out.size(0), -1).shape[1] # Keep the batch dimension intact and flatten out all of the other dimensions\n",
    "linear = nn.Linear(in_features=lin_input_features, out_features=3) # Set the input features equal to the flattened output features from convoluation layer\n",
    "                                                                   # Set the output features equal to the number of classes for prediction, 3 in this case\n",
    "linear_out = linear(conv_out.view(conv_out.size(0), -1)) # pass the flattened convolutional layer output through the lienar layer\n",
    "print(f\"Output shape of convolution layer: {conv_out.shape}\")\n",
    "print(f\"Input features to linear layer: {lin_input_features}\")\n",
    "print(f\"Output shape of the linear layer: {linear_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a technique used for regularisation in deep learning. The concept is simple, for each training sample batch / iteration, ignore a fraction (p) of units in a hidden layer. This can be intuitively considered as training using many many different models for each sample batch of data. The technique for dropout regularisation is demonstrated in the image below.\n",
    "\n",
    "<center>\n",
    "    <img src=./imgs/dropout.png height=700 width=700>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CharCNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CharCNN architecture used for this project very much follows the implementation (at least as a starting point) detailed in the [paper](https://arxiv.org/pdf/1509.01626.pdf) by Zhang et. al. The ConvNet structure is 9 layers deep and contains the following elements:\n",
    "\n",
    " - Input: (70 x 1014)\n",
    " - Conv1D(1): k=7, s=1, c_in=70, c_out=256, p='SAME'\n",
    "     - ReLu\n",
    "     - MaxPool1D: k=3, s=3\n",
    " - Conv1D(2): k=7, s=1, c_in=256, c_out=256, p='SAME'\n",
    "     - ReLu\n",
    "     - MaxPool1D: k=3, s=3\n",
    " - Conv1D(3): k=3, s=1, c_in=256, c_out=256, p='SAME'\n",
    "     - ReLu\n",
    " - Conv1D(4): k=3, s=1, c_in=256, c_out=256, p='SAME'\n",
    "     - ReLu\n",
    " - Conv1D(5): k=3, s=1, c_in=256, c_out=256, p='SAME'\n",
    "     - ReLu\n",
    " - Conv1D(6): k=3, s=1, c_in=256, c_out=256, p='SAME'\n",
    "     - ReLu\n",
    "     - MaxPool1D: k=3, s=3\n",
    " - Linear(7): c_in=9472, c_out=1024\n",
    "     - ReLu\n",
    "     - Dropout: p=0.5\n",
    " - Linear(8): c_in=1024, c_out=1024\n",
    "     - ReLu\n",
    "     - Dropout: p=0.5\n",
    " - Linear(9): c_in=1025, c_out=10\n",
    " \n",
    "***Note**:\n",
    " - The number of input features to the first linear layer is calculated by flattening the output matrix from the last convolution layer (excluding batch dimension).\n",
    " - There are 12,125,187 trainable parameters within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import rand, Tensor, nn, cuda\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Character level CNN implementation as per the paper authored by Zhang et.al. The paper \n",
    "    is titled \"Character-level convolutional Networks for Text Classification - 2016\". The \n",
    "    model consists of 9 layers including 6 convolutional layers and 3 fully connected layers.\n",
    "    Details are shown below.\n",
    "                \n",
    "                Conv Layers\n",
    "    -------------------------------------------------\n",
    "    Layer    Features    Kernel    Pool    Activation\n",
    "    -----    --------    ------    ----    ----------\n",
    "      1         256         7        3        ReLU\n",
    "      2         256         7        3        ReLU\n",
    "      3         256         3        N/A      ReLU \n",
    "      4         256         3        N/A      ReLU\n",
    "      5         256         3        N/A      ReLU\n",
    "      6         256         3        3        ReLU\n",
    "      \n",
    "                FC Layers\n",
    "    -----------------------------------\n",
    "    Layer    Features    Dropout    Activation\n",
    "    -----    --------    -------    ----------\n",
    "      7        1024        p=0.5       ReLU\n",
    "      8        1024        p=0.5       ReLU\n",
    "      9        TBC         N/A         N/A\n",
    "      \n",
    "    Furthermore, the paper initialised the model weights using a Gaussian distribution with \n",
    "    a standard deviation of 0.05. Note there were no specific details regarding padding of \n",
    "    the convolutional layer inputs, and 'SAME' padding is assumed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int, alph_len :int, max_len: int, num_classes: int):\n",
    "        \"\"\"\n",
    "        CharCNN model constructor. During model instantiation, the trainable model weights \n",
    "        are initialised using Gaussian distribution.\n",
    "        \n",
    "        :param batch_size: Number of input samples.\n",
    "        :type batch_size: int\n",
    "        :param alph_len: The number of characters in the alphabet for text quantisation. This \n",
    "            dictates the dimension of the first convolutional layer.\n",
    "        :type alph_len: int\n",
    "        :param max_len: The assume maximum input text length. This is the input features to \n",
    "            the model, and dictates the input feature length for the first linear layer.\n",
    "        :type max_len: int\n",
    "        :param num_classes: The number of classes for prediction. This dictates the number of \n",
    "            output units of the last linear layer.\n",
    "        :type num_classes: int\n",
    "        :return: Nothing.\n",
    "        :rtype: None\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CharCNN, self).__init__()\n",
    "    \n",
    "        # Convolutional layer architecture\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # conv1 -> (b x 256 x 338): \n",
    "            nn.Conv1d(alph_len, 256, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            # conv2 -> (b x 256 x 112)\n",
    "            nn.Conv1d(256, 256, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            # conv3 -> (b x 256 x 112)\n",
    "            nn.Conv1d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # conv4 -> (b x 256 x 112)\n",
    "            nn.Conv1d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # conv5 -> (b x 256 x 112)\n",
    "            nn.Conv1d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # conv6 -> (b x 256 x 37) -> 9472 features\n",
    "            nn.Conv1d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3)\n",
    "        )\n",
    "        \n",
    "        # Determine linear layer input shape\n",
    "        input_shape = (batch_size, alph_len, max_len)\n",
    "        self.lin_input_features = self._get_lin_features(input_shape)\n",
    "        \n",
    "        # Linear layer architecture\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            # linear7\n",
    "            nn.Linear(self.lin_input_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # linear8\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # linear9\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Calculate total number of trainable parameters in the model\n",
    "        self.total_params = self._get_total_params()\n",
    "        # Random initialisation of weights using gaussian distribution\n",
    "        self.apply(self._init_weights)\n",
    "        # If GPU is available, instantiate the model on GPU\n",
    "        if cuda.is_available():\n",
    "            self.cuda()\n",
    "    \n",
    "    def _get_lin_features(self, shape: Tuple[int]) -> int:\n",
    "        \"\"\"\n",
    "        Convenience function to calculate the input feature length for \n",
    "        the first linear layer in the model.\n",
    "        \n",
    "        :param shape: Input shape to the model in the form of (b x w x l) where \n",
    "            b, l and w are the input batch size, width (number of rows) and \n",
    "            length (number of columns) respectively.\n",
    "        :type shape: Tuple[int]\n",
    "        :return: The number of input features to the first linear layer in the \n",
    "            model.\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        x = rand(shape)\n",
    "        x = self.conv_layers(x)\n",
    "        return x.view(x.size(0), -1).shape[1]\n",
    "    \n",
    "    def _get_total_params(self) -> int:\n",
    "        \"\"\"\n",
    "        Convenience function for calculating the total number of trainable parameters \n",
    "        in the model.\n",
    "        \"\"\"\n",
    "        total_params = sum([p.numel() for p in self.parameters() \n",
    "                            if p.requires_grad])\n",
    "        return total_params\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module, mean: float=0., std: float=0.05) -> None:\n",
    "        \"\"\"\n",
    "        Convenience function for initialising the weights for a single module in the \n",
    "        model using Gaussian distribution with specified mean and standard deviation. \n",
    "        This function should be passed to nn.Module.apply which recursively applies\n",
    "        input function to all sub modules within the model.\n",
    "        \n",
    "        :param module: a nn.Module object to which the weight initialisation will be \n",
    "            applied.\n",
    "        :type module: nn.Module\n",
    "        :param mean: Mean for a Gaussian distribution.\n",
    "        :type mean: float\n",
    "        :param std: Standard deviation for a Gaussian distribution.\n",
    "        :type std: float\n",
    "        :return: Nothing. Modifies the input modules weights and biases inplace.\n",
    "        :rtype: None.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean, std) # inplace op with funct_\n",
    "            module.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation function for the model. The output of the model will \n",
    "        have dimension (batch_size x number of classes for prediction)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1,2) # transpose sicne input is actually provided as (max_len x alph_len) but we want the other way around\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the output from the convolutional layers, keep batch dimension intact\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: torch.Size([128, 1014, 70])\n",
      "Output dimension: torch.Size([128, 3])\n",
      "Number of trainable parameters in the model: 12,125,187\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "input_matrix = Tensor(np.random.randn(128, 1014, 70))\n",
    "model = CharCNN(128, 70, 1014, 3)\n",
    "output = model.forward(input_matrix)\n",
    "print(f\"Input dimension: {input_matrix.shape}\")\n",
    "print(f\"Output dimension: {output.shape}\")\n",
    "print(f\"Number of trainable parameters in the model: {model.total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optimisation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation algorithm consists of the following components (as per the paper):\n",
    " - Stochastic Gradient Descent with minibatch of size 128, using momentum 0.9.\n",
    " - Learning rate decay with initial step size of 0.01, halved every 3 epoches for 10 times.\n",
    " - Crossentropy loss function for training optimisation.\n",
    " - Evaluation metrics will be Accuracy and F1 score (to cater for imbalanced label class distribution in the training and test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it was seen that there are quite a bit of class imbalance in the dataset labels, it will be worthwhile to apply class weightings to the crossentropy loss function. **Focal Loss should be investigated in the future, it seems interesting [here](https://medium.com/@ayodeleodubela/what-does-focal-loss-mean-for-training-neural-networks-770636f76379).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [article](https://towardsdatascience.com/optimization-algorithms-in-deep-learning-191bfc2737a4) gives a pretty good treatment of the common optimisation algorithms used for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from collections import Counter\n",
    "from torch import cuda\n",
    "from pandas import read_csv, Series\n",
    "from typing import Union\n",
    "\n",
    "def get_class_weights(labels: Seires) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate class weightings based on each class' proportion\n",
    "    in the label.\n",
    "    \n",
    "    :param labels: The labels in the training dataset.\n",
    "    :type labels: Series\n",
    "    :return: A tensor of weights.\n",
    "    :rtype: Tensor\n",
    "    \"\"\"\n",
    "    # Calculate class weightings\n",
    "    class_counts = dict(Counter(train_labels))\n",
    "    m = max(class_counts.values())\n",
    "    for c in class_counts:\n",
    "        class_counts[c] = m / class_counts[c]\n",
    "    # Convert weightings to tensor\n",
    "    weights = []\n",
    "    for k in sorted(class_counts.keys()):\n",
    "        weights.append(class_counts[k])\n",
    "    weights = Tensor(weights)\n",
    "    # Move weights to GPU if available\n",
    "    if cuda.is_available():\n",
    "        weights = weights.cuda()\n",
    "    return weights\n",
    "\n",
    "def init_optimisation(model: CharCNN, \n",
    "                      optimiser: str='sgd', \n",
    "                      unbalance_classes: bool=False, class_weights: Tensor=None, \n",
    "                      lr: float=0.01, momentum: float=0.9, \n",
    "                      schedule_lr: bool=False) -> Tuple[Union[optim.SGD, optim.Adam], \n",
    "                                                        CrossEntropyLoss, MultiStepLR]:\n",
    "    \"\"\"\n",
    "    Initialise the optimisation algorithm which selects:\n",
    "    1. Balanced or unbalanced crossentropy loss function, if unbalanced, class weightings\n",
    "       are applied during loss optimisation.\n",
    "    2. Gradient descent algorithm ca be either SGD with momentum, or ADAM.\n",
    "    3. The user has the option to enable learning rate scheduling for the optimisation \n",
    "       algorithm. The scheduler implements learning rate reduction by halving it every \n",
    "       three epochs up to the point where this has been applied 10 times.\n",
    "       \n",
    "    :param model: A CharCNN model from which parameters will be updated by the optimiser.\n",
    "    :type model: CharCNN\n",
    "    :param optimiser: The type of gradient descent algorithm to use. Can be 'sgd' or 'adam'.\n",
    "    :type optimiser: str\n",
    "    :param unbalance_classes: Indicator for initialising a weighted crossentropy loss function.\n",
    "    :type unbalance_classes: bool\n",
    "    :param class_weights: The list of weightings to be applied to each class for the crossentropy\n",
    "        loss function. Note if unbalance_classes is False, this parameter will be ignored.\n",
    "    :type class_weights: Tensor\n",
    "    :param lr: learning rate for the gradient descent. If schedule_lr is True, this will be \n",
    "        the initial learning rate.\n",
    "    :type lr: float\n",
    "    :param momentum: the velocity coefficient gradient descent with momentum\n",
    "    :type momentum: float\n",
    "    :param schedule_lr: Indicator to enable learning rate scheduling with the algorithm mentioned \n",
    "        in the function description.\n",
    "    :type schedule_lr: bool\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Balance or unbalanced loss function\n",
    "    if unbalance_classes:\n",
    "        criterion = CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        criterion = CrossEntropyLoss() # The cross_entropy function includes softmax calc, and can have weights applied for different classes\n",
    "    \n",
    "    # Choose optimiser\n",
    "    if optimiser == 'sgd':\n",
    "        optimiser = optim.SGD(model.parameters(), lr=lr, momentum=momentum) # Stochastic gradient descent optimiser, can have momentum term passed to it\n",
    "    elif optimiser == 'adam':\n",
    "        optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "    # Create learning rate scheduler\n",
    "    if schedule_lr:\n",
    "        # Multiply optimiser learning rate by 0.5 for each milestone epochs specified in steps\n",
    "        steps = [x*3 for x in range(1, 11)]\n",
    "        scheduler = MultiStepLR(optimiser, milestones=steps, gamma=0.5)\n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return optimiser, criterion, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimisation\n",
    "df = read_csv(\"../data/test/test_clean.csv\")\n",
    "train_labels = df['rating']\n",
    "class_weights = get_class_weights(train_labels)\n",
    "optimiser, criterion, scheduler = init_optimisation(model, 'sgd', unbalance_classes=True, \n",
    "                                                    class_weights=class_weights, schedule_lr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Results\n",
    "\n",
    "The training of the model was conducted on a GPU with the following settings for the final model:\n",
    "\n",
    " - **Quantisation**\n",
    "  - Alphabet: (abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\\n)\n",
    "  - Max Input Size: 1014 characters.\n",
    "  \n",
    "  \n",
    " - **Model parameters**\n",
    "  - Refer to this [paper](https://arxiv.org/pdf/1509.01626.pdf) for model layer attributes.\n",
    "  - Output classes: 0 - poor; 1 - average; 2 - good\n",
    "  \n",
    "  \n",
    " - **Data parameters**\n",
    "  - Batch size: 1024 samples\n",
    "  - Sampler: random weighted sampling for each batch (used to deal with unbalanced classes)\n",
    "  - Multiprocessing: 2 concurrent processes for data loading.\n",
    "  \n",
    "  \n",
    " - **Training parameters**\n",
    "  - Optimiser: Stochastic Gradient Descent with Momentum.\n",
    "   - Learning rate (initial): 0.01\n",
    "   - Momentum: 0.9\n",
    "   - Learning rate scheuling: Stepped (halves every 3 epochs for a maximum of 10 times if it gets there)\n",
    "  - Number of epochs: 10\n",
    "  - Loss function: Weighted crossentropy.\n",
    "  - Early-stopping: Enabled (waits 3 epochs for test f1 score improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Logged results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the model ended on the 7th epoch out of the 10 epochs defined.The final results is shown below:\n",
    "\n",
    "<img src=./imgs/results.png height=400 width=400>\n",
    "\n",
    "<img src=./imgs/train_pic.png height=600 width=1000>\n",
    "\n",
    "<img src=./imgs/test_pic.png height=600 width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training results, the below observations are made:\n",
    "\n",
    " - The overall accuracy of the model on the last epoch is `0.84`\n",
    " - Predictions performs the best on class `2` worst on class `1`. This is due to the class imbalance in the data mainly, but intuitively it's also much more difficult to gauge an `average` sentiment since it would include much more nuance from a language perspective. Getting more data will certianly help, but we can also maybe change the model to treat a binary classification problem.\n",
    " - The training scores are reasonably close to the testing scores, which means the model is not overfitting (except for f1 score on class 1 predictions).\n",
    " - The test scores reached a plateau much earlier than training scores, in this case, the early stopping mechanism seems to have aided the training process. Since if the training carried on, we will probably start seeing the test scores diverge with respect to the training scores i.e. model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Play with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide a input sentence:  This product sucks!! @Puma #notgood\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Poor\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Make another prediction? [Y/N]\n",
      " y\n",
      "Please provide a input sentence:  I guess the service is okay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Average\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Make another prediction? [Y/N]\n",
      " So the other day I bought this new bike, and it was awesome!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not enter a valid option!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Make another prediction? [Y/N]\n",
      " y\n",
      "Please provide a input sentence:  So the other day I bought this new bike, and it was awesome!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Good\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Make another prediction? [Y/N]\n",
      " y\n",
      "Please provide a input sentence:  I'm a bit uncertain about the project status, can we arrange a meeting soon?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Average\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Make another prediction? [Y/N]\n",
      " y\n",
      "Please provide a input sentence:  Coffee is a brewed drink prepared from roasted coffee beans, the seeds of berries from certain Coffea species. When coffee berries turn from green to bright red in color – indicating ripeness – they are picked, processed, and dried.[2] Dried coffee seeds (referred to as \"beans\") are roasted to varying degrees, depending on the desired flavor. Roasted beans are ground and then brewed with near-boiling water to produce the beverage known as coffee. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Average\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Make another prediction? [Y/N]\n",
      " n\n"
     ]
    }
   ],
   "source": [
    "from predict import load_model, infer\n",
    "\n",
    "while True:\n",
    "    input_str = input(\"Please provide a input sentence: \")\n",
    "    model = load_model()\n",
    "    print(infer(input_str, model))\n",
    "    next_str = input(\"Make another prediction? [Y/N]\\n\")\n",
    "    if next_str.lower() not in ('y', 'n'):\n",
    "        print(\"Did not enter a valid option!\")\n",
    "        next_str = input(\"Make another prediction? [Y/N]\\n\")\n",
    "    elif next_str == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
