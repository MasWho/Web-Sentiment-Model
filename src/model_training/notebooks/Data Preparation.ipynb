{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Before training the model, the data must be processed so that it can be in the acceptable form when feeding to the neural net. Also, some cleaning and wrangling will be required for the data. The following things should be considered:\n",
    "\n",
    " - Cleaning / preprocessing data:\n",
    "     - remove emojis (apply to inputs)\n",
    "     - remove hashtags (apply to inputs)\n",
    "     - remove user mentions (apply to inputs)\n",
    "     - remove urls (apply to inputs)\n",
    "     - convert text to lower case (apply to inputs)\n",
    "     - remove rows with a rating that is not a integer between 1 - 5 (don't apply to inputs)\n",
    " - Dealing with large data set, batch processing.\n",
    " - Determine label / sentiment distribution and try to sample as evenly as possible accross the different classes i.e. good, average, poor.\n",
    " - Quantization of text for inputing into the neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning / Preprocessing data\n",
    "\n",
    "This [Medium article](https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d) had detailed implementations for many text preprocessing tasks, including removing emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data_dir = '../data/raw/scraped_data.csv'\n",
    "# Read data in as chunks, and keep only relevant columns i.e. text comment, and label rating\n",
    "df = pd.read_csv('../data/raw/scraped_data.csv', \n",
    "                 chunksize=100_000, \n",
    "                 usecols=['comment', 'rating'],\n",
    "                 encoding='utf-8',\n",
    "                 nrows=None,\n",
    "                 sep=',')\n",
    "\n",
    "def check_basic_data_stats(data):\n",
    "    \n",
    "    len_data = 0\n",
    "    memory_usage = pd.Series(dtype='object')\n",
    "    \n",
    "    for chunk in data:\n",
    "        # Determine length of dataset\n",
    "        len_data += len(chunk)\n",
    "    \n",
    "        # Determine memory usage of dataframe\n",
    "        if len(memory_usage) == 0:\n",
    "            memory_usage = chunk.memory_usage(deep=True)/1e6\n",
    "        else:\n",
    "            memory_usage += chunk.memory_usage(deep=True)/1e6\n",
    "\n",
    "    print(f'Number of reviews scraped from Trustpilot: {len_data:,} reviews\\n')\n",
    "    print(f'Memory usage per column (MB):\\n{memory_usage}\\n')\n",
    "    print(f'Total memory usage of dataframe: {memory_usage.sum()/1e3:.2f} GB\\n')\n",
    "    return chunk # this is the last chunk in the entire data set, return for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews scraped from Trustpilot: 5,980,003 reviews\n",
      "\n",
      "Memory usage per column (MB):\n",
      "Index         0.007916\n",
      "comment    2006.976003\n",
      "rating       47.840024\n",
      "dtype: float64\n",
      "\n",
      "Total memory usage of dataframe: 2.05 GB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5900000</th>\n",
       "      <td>excellent, love being able to read the bottle ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900001</th>\n",
       "      <td>Fast delivery,love the items I've ordered &amp; fa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900002</th>\n",
       "      <td>So simple and easy to shop on line,I will orde...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900003</th>\n",
       "      <td>Great bargains and quick delivery.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900004</th>\n",
       "      <td>Grandson loved the books.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   comment  rating\n",
       "5900000  excellent, love being able to read the bottle ...       5\n",
       "5900001  Fast delivery,love the items I've ordered & fa...       5\n",
       "5900002  So simple and easy to shop on line,I will orde...       5\n",
       "5900003                 Great bargains and quick delivery.       5\n",
       "5900004                          Grandson loved the books.       4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_basic_data_stats(df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Removing emojis\n",
    "\n",
    "Decided not to remove emoticons, since the character level CNN will learn the pattern in the emoticons. The reason to remove emojis is due to their encodings covering too wide of a range. In the future, maybe we can identify a few emojis that can be included in the input features since they do represent sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am Emoji\n",
      "No emojis here\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to remove emoji.\n",
    "def remove_emoji(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove emojis from a input string of text.\n",
    "    \n",
    "    :param text: text to be processed.\n",
    "    :type text: str\n",
    "    :returns: processed text with emojis removed.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text).strip()\n",
    "\n",
    "# Test function\n",
    "print(remove_emoji(\"Hi, I am Emoji  ðŸ˜œ\"))\n",
    "print(remove_emoji(\"No emojis here\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Removing hashtags\n",
    "\n",
    "Regex [cheat sheet](https://www.rexegg.com/regex-quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am    should not remove\n",
      "should  ?? remove\n",
      "No hashtags here\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_hashtags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove hashtags from the input text. e.g. #happy\n",
    "    \n",
    "    :param text: text to be processed.\n",
    "    :type text: str\n",
    "    :returns: processed text with hashtags removed.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    clean_text = re.sub(r'#[A-Za-z0-9_]+', \"\", text)\n",
    "    return clean_text.strip()\n",
    "\n",
    "# Test function\n",
    "print(remove_hashtags(\"I am #happy #sad #ilovecoffee should not remove\"))\n",
    "print(remove_hashtags(\" #happy #sad #ilovecoffee \\nshould #not ?? remove\"))\n",
    "print(remove_hashtags(\"No hashtags here\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Removing user mentions\n",
    "\n",
    "Regex [cheat sheet](https://www.rexegg.com/regex-quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", is this how you review products?\n",
      "hey, users ,  and\n",
      "No user mentions here\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_user_mentions(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove user mentions from the input text. e.g. @mason\n",
    "    \n",
    "    :param text: text to be processed.\n",
    "    :type text: str\n",
    "    :returns: processed text with user mentions removed.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    clean_text = re.sub(r'@[A-Za-z0-9_]+', \"\", text)\n",
    "    return clean_text.strip()\n",
    "\n",
    "# Test function\n",
    "print(remove_user_mentions(\"@mason, is this how you review products?\"))\n",
    "print(remove_user_mentions(\"hey, users @user1, @user2 and @user3\"))\n",
    "print(remove_user_mentions(\"No user mentions here\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Removing urls\n",
    "\n",
    "Regex [cheat sheet](https://www.rexegg.com/regex-quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this website:\n",
      "Go to this website:\n",
      "No url here\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove ulrs from the input text. e.g. https:\\\\www.google.com\n",
    "    \n",
    "    :param text: text to be processed.\n",
    "    :type text: str\n",
    "    :returns: processed text with urls removed.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    clean_text = re.sub(r'http\\S+', '', text) # urls can span across many lines\n",
    "    return clean_text.strip()\n",
    "\n",
    "# Test function\n",
    "print(remove_urls(\"Go to this website: https://trello.com/c/eZwkuQE1/3-phase-2-model-training\"))\n",
    "print(remove_urls(\"Go to this website: https://www.youtube.com/watch?v=WgAIi1c369k\"))\n",
    "print(remove_urls(\"No url here\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Converting to lower case\n",
    "\n",
    "Lets assume everyone knows how to dow this one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text) -> str:\n",
    "    \"\"\"\n",
    "    Convert input string to lower case.\n",
    "    \n",
    "    :param text: text to be processed.\n",
    "    :type text: str\n",
    "    :returns: processed text converted to lower case.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Filtering out data with labels not a integer in the range of 1 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "def clean_label(batch: DataFrame, col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Take an input dataframe (probably part of a batch process) and make sure that \n",
    "    the label columns contain all integers in the range of 1 - 5.\n",
    "    \n",
    "    :param batch: a dataframe.\n",
    "    :type batch: DataFrame\n",
    "    :param col: the label column.\n",
    "    :type col: str\n",
    "    :returns: a processed dataframe with the label column cleaned.\n",
    "    :rtype: DataFrame\n",
    "    \"\"\"\n",
    "    # First check if the label column has dtype of int\n",
    "    int_check = batch[col].dtype == 'int'\n",
    "    # If the column is already in integer format\n",
    "    # only check if values are within range\n",
    "    if int_check:\n",
    "        batch = batch[batch[col].isin(range(1,6))]\n",
    "    # If the column isn't in integer format\n",
    "    # Filter out rows where label is not a number\n",
    "    # Then check if values are within range\n",
    "    else:\n",
    "        num_rows = batch.shape[0]\n",
    "        batch = batch[batch[col].str.isnumeric()]\n",
    "        batch = batch.astype('int')\n",
    "        batch = batch[batch[col].isin(range(1,6))]\n",
    "        print(f\"Found bad label: removed {batch.shape[0]-num_rows} rows.\")\n",
    "    return batch\n",
    "\n",
    "# Testing function\n",
    "# df = read_csv('../data/raw/scraped_data.csv', \n",
    "#                  chunksize=100_000, \n",
    "#                  usecols=['comment', 'rating'],\n",
    "#                  encoding='utf-8',\n",
    "#                  nrows=None,\n",
    "#                  sep=',')\n",
    "# for chunk in df:\n",
    "#     clean_label(chunk, 'rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Create pipeline for preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Iterator, Iterable, Generator\n",
    "from pandas import DataFrame, read_csv\n",
    "from pandas.io.parsers import TextFileReader\n",
    "\n",
    "def preprocess_text(data: Union[str, DataFrame, TextFileReader], \n",
    "                    text_col: str='comment', \n",
    "                    label_col: str='rating') -> Union[str, DataFrame, Generator[DataFrame, None, None]]:\n",
    "    \"\"\"\n",
    "    Applies preprocessing to input data. Allowed data types can be a input string of text, \n",
    "    a pandas DataFrame or a pandas TextFileReader. The latter input type generally occrurs \n",
    "    if the dataset is large and won't fit in memory as a whole. In that case, the returned \n",
    "    data is a generator. This can be thought of as chaining generators to produce a stream \n",
    "    of data that will fit in memory.\n",
    "    \n",
    "    :param data: The input text data to be preprocessed.\n",
    "    :type data: Union[str, DataFrame, TextFileReader]\n",
    "    :param text_col: The column of dataframe that contains the text data. If input is a \n",
    "                     single string, this parameter is ignored. Defaults to 'comment'\n",
    "                     corresponding to the original dataset column name.\n",
    "    :type text_col: str\n",
    "    :param label_col: The column of dataframe that contains the label data. If input is a \n",
    "                      single string, this parameter is ignored. Defaults to 'rating'\n",
    "                      corresponding to the original dataset column name.\n",
    "    :type label_col: str\n",
    "    :raise: TypeError if the input data format isn't either a str, DataFrame or TextFileReader.\n",
    "    :returns: Preprocessed text data.\n",
    "    :rtypes: Union[str, DataFrame, Generator[DataFrame, None, None]] \n",
    "    \"\"\"\n",
    "    preprocessing_steps = {\n",
    "        'Removing emojis': remove_emoji,\n",
    "        'Removing hashtags': remove_hashtags,\n",
    "        'Removing user mentions': remove_user_mentions,\n",
    "        'Removing urls': remove_urls,\n",
    "        'Converting to lower case': lower\n",
    "    }\n",
    "    \n",
    "    def preprocess_text_chunk(chunk: DataFrame) -> DataFrame:\n",
    "        # Apply the preprocessing pipeline to a dataframe\n",
    "        for step, func in preprocessing_steps.items():\n",
    "            chunk[text_col] = chunk[text_col].apply(func)\n",
    "        chunk = chunk.dropna()\n",
    "        chunk = clean_label(chunk, label_col)\n",
    "        return chunk\n",
    "    \n",
    "    # If input is a single instance of text, then we are making inference\n",
    "    if isinstance(data, str):\n",
    "        for step, func in preprocessing_steps.items():\n",
    "            data = func(data)\n",
    "        return data\n",
    "    \n",
    "    # If input is a dataframe, clean the dataframe and return it\n",
    "    elif isinstance(data, DataFrame):\n",
    "        return preprocess_text_chunk(data)\n",
    "\n",
    "    # If input is a TextFileReader, \n",
    "    # clean it but return a generator to improve memory efficiency\n",
    "    elif isinstance(data, TextFileReader):\n",
    "        return (preprocess_text_chunk(chunk) for chunk in data)\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"\"\"Allowed types are one of the following:\n",
    "            - str, \n",
    "            - pandas.DataFrame\n",
    "            - pandas.io.parsers.TextFileReader\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Functions\n",
    "df = read_csv('../data/raw/scraped_data.csv', \n",
    "                 chunksize=100_000, \n",
    "                 usecols=['comment', 'rating'],\n",
    "                 encoding='utf-8',\n",
    "                 nrows=None,\n",
    "                 sep=',')\n",
    "# chunk = next(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_chunk = preprocess_text(chunk)\n",
    "clean_df = preprocess_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will first clean and preprocess the data, then we need to prepare the data for training i.e. splitting the dataset into training and testing set (not using a validation set in this case). This will be needed only for training purposes, when making inference, there is no need to split the input and therefore we will not be expecting strings but dataframes or generators as input for this part of the pipeline.\n",
    "\n",
    "Note that the raw data contains 5 classes within the label i.e. 1 - 5 star ratings. For the purpose of this project, we will group \\[1,2\\] stars to 'poor', 3 stars to 'average' and \\[4,5\\] stars to 'good'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Determine Distribution of label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Generator\n",
    "from pandas import DataFrame, Series\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from copy import deepcopy\n",
    "\n",
    "def group_labels(data: Union[DataFrame, Generator[DataFrame, None, None]], \n",
    "                 label_col: str='rating') -> Union[DataFrame, Generator[DataFrame, None, None]]:\n",
    "    \"\"\"\n",
    "    Assign new groups to old label classes.\n",
    "    \"\"\"\n",
    "    def _assign_class(data: DataFrame):\n",
    "        # Assign group of labels to a new set of classes 0,1,2 representing poor, average and bad\n",
    "        data.loc[(data[label_col]==1) | (data[label_col]==2), label_col] = 0 # Group [1,2] to 0\n",
    "        data.loc[(data[label_col]==3), label_col] = 1 # Group [3] to 1\n",
    "        data.loc[(data[label_col]==4) | (data[label_col]==5), label_col] = 2 # Group [4,5] to 2\n",
    "        return data\n",
    "    \n",
    "    if isinstance(data, DataFrame):\n",
    "        return _assign_class(data)\n",
    "    \n",
    "    elif isinstance(data, Generator):\n",
    "        return (_assign_class(chunk) for chunk in data)\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"\"\"Allowed types are one of the following: \n",
    "            - pandas.DataFrame\n",
    "            - Generator[pandas.DataFrame, None, None]\"\"\")\n",
    "\n",
    "def label_class_fcount(data: Union[DataFrame, Generator[DataFrame, None, None]], \n",
    "                       label_col: str='rating') -> DataFrame:\n",
    "    \"\"\"\n",
    "    Take either a dataframe or a generator of dataframes and return a Series representing \n",
    "    the frequency count of existing label classes. In addition, also calculate the corresponding \n",
    "    probability of each class.\n",
    "    \n",
    "    :param data: the data containing the label column for which class frequency count \n",
    "                 must be determined.\n",
    "    :type data: Union[DataFrame, Generator[DataFrame, None, None]]\n",
    "    :param label_col: The coloumn in the data for which frequency count must be determined.\n",
    "                      Defaults to 'rating' corresponding ot the label column in the original data.\n",
    "    :type label_col: str\n",
    "    :raise: TypeError if the input data format isn't either a DataFrame or Generator[DataFrame, None, None].\n",
    "    :return: frequency count of all distinct classes within the label data as well as their corresponding probabilities.\n",
    "    :rtype: DataFrame\n",
    "    \"\"\"\n",
    "    data = deepcopy(data)\n",
    "    if isinstance(data, DataFrame):\n",
    "        fcount = data[label_col].value_counts()\n",
    "        sorted_idx = fcount.index.sort_values()\n",
    "        fcount = fcount[sorted_idx]\n",
    "        prob = fcount / fcount.sum()\n",
    "        return DataFrame({'frequency': fcount, 'probability': prob})\n",
    "    \n",
    "    elif isinstance(data, Generator):\n",
    "        fcount_total = None\n",
    "        for chunk in data:\n",
    "            if fcount_total is None:\n",
    "                fcount_total = chunk[label_col].value_counts()\n",
    "            else:\n",
    "                fcount_total += chunk[label_col].value_counts()\n",
    "        sorted_idx = fcount_total.index.sort_values()\n",
    "        fcount_total = fcount_total[sorted_idx]\n",
    "        prob = fcount_total / fcount_total.sum()\n",
    "        return DataFrame({'frequency': fcount_total, 'probability': prob})\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"\"\"Allowed types are one of the following: \n",
    "            - pandas.DataFrame\n",
    "            - Generator[pandas.DataFrame, None, None]\"\"\")\n",
    "        \n",
    "def plot_label_dist(label_dist: DataFrame, plot_type: str='frequency') -> None:\n",
    "    \"\"\"\n",
    "    Helper function to plot the label class distribution.\n",
    "    \n",
    "    :param label_dist: the label class distribution summary.\n",
    "    :type label_dist: DataFrame\n",
    "    :param plot_type: the type of plot to show. Can be 'frequency' or 'probability'.\n",
    "    :type plot_type: str\n",
    "    :return: Nothing\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    ax = label_dist[plot_type].plot.bar(figsize=(10,5))\n",
    "    ax.grid()\n",
    "    ax.get_yaxis().set_major_formatter(FuncFormatter(lambda x, p: format(x, ',.2f'))) # Set y axis 1000s separator\n",
    "    ax.tick_params(axis='x', labelrotation=0) \n",
    "    ax.set_xlabel('Label Classes')\n",
    "    ax.set_ylabel(plot_type)\n",
    "    ax.set_title(f\"Label class {plot_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data with different labels assigned to new classes\n",
    "clean_df = group_labels(clean_df, 'rating')\n",
    "\n",
    "# Get frequency count and probablity of each label class\n",
    "# label_fcount = label_class_fcount(clean_df, 'rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frequency  probability\n",
      "0     570201     0.096973\n",
      "1     234677     0.039911\n",
      "2    5075125     0.863116\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(label_fcount)\n",
    "plot_label_dist(label_fcount, plot_type='probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution above, it's quite clear that most reviews on trustpilot are good reviews with 5 starts. If we group \\[4,5\\] as 'good', 3 as 'average' and \\[1,2\\] as 'poor', the respective distribution across the entire dataset for these classes will be 0.86, 0.04 and 0.1. This is of course not ideal for the following obvious reasons:\n",
    "\n",
    " - The model trained on this data might overfit to the good reviews.\n",
    " - If we don't sample properly, there is a good chance that we might end up with a training set that contains (let's be extreme) 99% good reviews, this will result in the model having poor generalisation for predicting average and poor reviews.\n",
    " - It's important that we ensure enough data from each label class is in the test set to accurately evaluate the model's performance.\n",
    " \n",
    "With the above in mind, the following approach will be used for data sampling and train / test split:\n",
    " - Stratified sampling will be used i.e. random sampling in each sub group of the dataset each corresponding to a single label class.\n",
    " - A single train / test split ratio will be defined, but this ratio will be applied individually to the sub group of the dataset each corresponding to a single label class.\n",
    " - All available data from the 'poor' (0) and 'average' (1) classes will be used, and 16% samples from the 'good' (2) class (~800k) will be used which is the amount of samples from the first two classes combined.\n",
    " \n",
    "This approach is a initial starting point, it might be worth tweaking to get improve on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train test split of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Generator, Tuple, List\n",
    "from pandas import DataFrame, concat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "def raw_train_test_split(data: Union[DataFrame, Generator[DataFrame, None, None]], \n",
    "                         train_size: float=0.9, text_col: str='comment', label_col: str='rating',\n",
    "                         class_data_usage: List[float]=[1,1,0.16],\n",
    "                         classes: List[int]=[0,1,2]) -> Union[Tuple[DataFrame, DataFrame], \n",
    "                                                              Tuple[Generator[DataFrame, None, None], \n",
    "                                                                    Generator[DataFrame, None, None]]]:\n",
    "    \"\"\"\n",
    "    Take a dataset and split it into training and test set. Flexibility is provided that the user may \n",
    "    provide the possible label classes that exist in the dataset, and specify the percentage data usage \n",
    "    on a per class basis. For example, given a data set that contains a binary label with distinct values \n",
    "    [1, 2], the user can specify the percentage of data to be used per class for constructing the training \n",
    "    and test set. If in this scenario the user would like to use 50% and 80% of data corresponding to the two \n",
    "    label classes, the user may pass a list of proportions e.g [0.5, 0.8] to the class_data_usage parameter.\n",
    "    \n",
    "    :param data: the input dataset to be split into train and test set.\n",
    "    :type data: Union[DataFrame, Generator[DataFrame, None, None]]\n",
    "    :param train_size: proportion for the training set in the range between 0 and 1. Default to 0.9.\n",
    "    :type train_size: float\n",
    "    :param text_col: the column name of the text.\n",
    "    :type text_col: str\n",
    "    :param label_col: the column name of the label.\n",
    "    :type label_col: str\n",
    "    :param class_data_usage: a list of usage proportions for the existing sub datasets each corresponding to a \n",
    "                             label class. Individual values should be in the range of 0-1, and the elements in  \n",
    "                             the list must correlate to the elements in classes list.\n",
    "    :type class_data_usage: List[float]\n",
    "    :param classes: a list of label classes that exist in the dataset. The elements in the lsit must correlate \n",
    "                    to the elements in the class_data_usage list.\n",
    "    :return: the split train and test set. Note this function allows output as a generator to reduce memory usage.\n",
    "             The output data type is directly dependent on the input datatype.\n",
    "    :rtype: Union[Tuple[DataFrame, DataFrame], \n",
    "                  Tuple[Generator[DataFrame, None, None], Generator[DataFrame, None, None]]]:\n",
    "    \"\"\"\n",
    "    def _raw_train_test_split(data: Union[DataFrame, Generator[DataFrame, None, None]]):\n",
    "        train_set = []\n",
    "        test_set = []\n",
    "        for class_, usage in zip(classes, class_data_usage):\n",
    "            class_split = data[data[label_col]==class_]\n",
    "            class_train, class_test = train_test_split(class_split, train_size=train_size, random_state=10)\n",
    "            class_train = class_train.iloc[:int(usage*class_train.shape[0]), :] # Grab proportion of data based on class usage specified\n",
    "            class_test = class_test.iloc[:int(usage*class_test.shape[0]), :]\n",
    "            train_set.append(class_train)\n",
    "            test_set.append(class_test)\n",
    "        return concat(train_set), concat(test_set)\n",
    "    \n",
    "    if isinstance(data, DataFrame):\n",
    "        train_set, test_set = _raw_train_test_split(data)\n",
    "        return train_set, test_set\n",
    "    \n",
    "    elif isinstance(data, Generator):\n",
    "        train_test_gen = (_raw_train_test_split(chunk) for chunk in data)\n",
    "        return train_test_gen\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"\"\"Allowed types are one of the following: \n",
    "            - pandas.DataFrame\n",
    "            - Generator[pandas.DataFrame, None, None]\"\"\")\n",
    "        \n",
    "def save_train_test_data(data: Generator[DataFrame, None, None], \n",
    "                         train_path: str, test_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the train and test datasets to their respective folders. This function is \n",
    "    specifically written to save generator data to csv files.\n",
    "\n",
    "    :param data: the generator that will return train and test sets.\n",
    "    :type data: Generator[DataFrame, None, None]]\n",
    "    :param train_path: path to which the training set must be saved.\n",
    "    :type train_path: str\n",
    "    :param test_path: path to which the test set must be saved.\n",
    "    :type test_path: str\n",
    "    :return: Nothing.\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    final_train_set = []\n",
    "    final_test_set = []\n",
    "    for train_set, test_set in tqdm(data, total=59):\n",
    "        final_train_set.append(train_set)\n",
    "        final_test_set.append(test_set)\n",
    "    final_train_set = concat(final_train_set)\n",
    "    final_test_set = concat(final_test_set)\n",
    "    final_train_set.to_csv(train_path, index=False)\n",
    "    final_test_set.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [03:16,  3.27s/it]                        \n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "train_path = '../data/train/train_clean.csv'\n",
    "test_path = '../data/test/test_clean.csv'\n",
    "df = read_csv('../data/raw/scraped_data.csv', \n",
    "                 chunksize=100_000, \n",
    "                 usecols=['comment', 'rating'],\n",
    "                 encoding='utf-8',\n",
    "                 nrows=None,\n",
    "                 sep=',')\n",
    "clean_df = preprocess_text(df)\n",
    "clean_df = group_labels(clean_df, 'rating')\n",
    "clean_train_test_df = raw_train_test_split(clean_df)\n",
    "save_train_test_data(clean_train_test_df, train_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the trainng and testing sets have been prepared. However, before we actually feed the data into the model for training, there is one more thing to do which is the quantisation of text data. To understand this step, it is necessary to describe the neural network architecture first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization of text data for character level CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing tasks has been a hot topic in the deep learning field. There are many models available out there for NLP which have been detailed in this [article](https://medium.com/capital-one-tech/the-nlp-playbook-part-1-deep-dive-into-text-classification-61503144de78) on Medium. This is displayed in the image below.\n",
    "\n",
    "<center><img src=./imgs/img1.png height=500 width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Character level CNN or Char-CNN is chosen for the job here due to its advantage of less requirements on text preprocessing, and its ability to handle out of vocabulary / mis-spelled words since the model treats texts similar to signals. Although Char-CNN performs poorly for long length text data, in this case since we are working with reviews, we won't expect too many instances where the text input is very lengthy.\n",
    "\n",
    "The Char-CNN was proposed in this [paper](https://arxiv.org/pdf/1509.01626.pdf) by Zhang et. al. The crucial aspects allowing the success of Char-CNNs, as detailed in the paper, can be prescribed to the representation of text input as well as the application of 1-D convolutions to capture temporal (sequential) structure of the text data.\n",
    "\n",
    "We will talk about the representation of text input here since it forms part of the data preparation process. In order to efficiently use text data for the Char-CNN, the text must be transformed using **quantisation**. This transformation is simply demonstrated by the animation below.\n",
    "\n",
    "<center><img src=./imgs/img2.gif width=600 height=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, to carry out **quantisation**, we will need an alphabet and assume a maximum text input length (number of characters). Given an actual group of text, we can then construct a matrix that contains all the input characters as one-hot encoded vectors where each column corresponds to a single character from the input text. The number of columns and rows corresponds to the number of characters in the alphabet and the assumed maximum text input length respectively. Therefore, the dimension of this matrix is fixed regardless of the input text size.\n",
    "\n",
    "Note if there are more characters in the input text than the assumed maximum text input length, then the input will be truncated to the maximum input length. Conversly, if there are less characters in the input text than the assumed maximum text input length, then zero padding will be applied until the entire matrix is filled up.\n",
    "\n",
    "The alphabet and maximum input length chosen here are the same as detailed in the paper authored by Zhang et. al:\n",
    " - Alphabet: abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\n",
    " - Maximum length: 1014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Generator\n",
    "from pandas import read_csv, DataFrame, Series, concat\n",
    "from pandas.io.parsers import TextFileReader\n",
    "from numpy import identity, zeros\n",
    "\n",
    "def quant_text(data: Union[str, Series, TextFileReader],\n",
    "               text_col: str='comment',\n",
    "               alph: str=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\", \n",
    "               max_length: int=1014) -> Union[DataFrame, Series, \n",
    "                                              Generator[Series, None, None]]:\n",
    "    \"\"\"\n",
    "    Takes an input text set (can be a single text input), and returns the quantised version of all its \n",
    "    elements.\n",
    "    \n",
    "    :param data: text input on which quantisation shall be performed.\n",
    "    :type data: Union[str, Series, TextFileReader]\n",
    "    :param text_col: the text column of the input TextFileReader. This parameter \n",
    "                     will be ignored in case that the data input format is not a \n",
    "                     pandas TextFileReader object.\n",
    "    :type text_col: str\n",
    "    :param alph: the alphabet to be used for text quantisation. It is critical to escape special characters.\n",
    "    :type alph: str\n",
    "    :param max_length: the maximum text input length used for either truncating or padding of the \n",
    "                       quantised text matrix.\n",
    "    :type max_length: int\n",
    "    :return: quantised text.\n",
    "    :rtype: Union[DataFrame, Series[DataFrame], Generator[Series[DataFrame], None, None]]\n",
    "    \"\"\"\n",
    "    # Define identiy matrix for the alphabet\n",
    "    # Each row/col is a one-hot vector for a character in the alphabet\n",
    "    identity_mat = identity(len(alph))\n",
    "    identity_mat = DataFrame(identity_mat, dtype='float32', index=list(alph), columns=list(alph))\n",
    "    \n",
    "    def quant_single_text(data: str):\n",
    "        text_mat = None\n",
    "        # Catch scenarios where text is an empty string, do nothing in this case\n",
    "        if len(data) > 0:\n",
    "            # Character vectors need to be constructed in reverse order\n",
    "            # This is to feed data into the CNN in the right order.\n",
    "            reversed_text = data[::-1]\n",
    "            # Create quantised text matrix with:\n",
    "            # columns: characters in alphabet\n",
    "            # rows: characters in input string, fixed size to max_length\n",
    "            text_mat = concat([identity_mat.loc[c] for c in reversed_text \n",
    "                               if c in alph], axis=1)\n",
    "            text_mat = text_mat.transpose().reset_index(drop=True)\n",
    "            \n",
    "            # Truncate text matrix if input text length is more than max length\n",
    "            if text_mat.shape[0] > max_length:\n",
    "                text_mat = text_mat.iloc[:max_length]\n",
    "\n",
    "            # Pad text matrix if input text length is less than max length\n",
    "            elif 0 < text_mat.shape[0] < max_length:\n",
    "                diff = max_length - text_mat.shape[0]\n",
    "                df_to_append = DataFrame(zeros((diff, len(alph))), columns=list(alph))\n",
    "                text_mat = concat([text_mat, df_to_append], axis=0).reset_index(drop=True)\n",
    "        return text_mat\n",
    "    \n",
    "    # If input is a single text, then output its corresponding quantised matrix\n",
    "    if isinstance(data, str):\n",
    "        return quant_single_text(data)\n",
    "    \n",
    "    # If input is a series of texts, then output a series with corresponding quantised matrices\n",
    "    elif isinstance(data, Series):\n",
    "        return data.apply(quant_single_text)\n",
    "    \n",
    "    # If input is a pandas TextFileReader object, then output a generator\n",
    "    # The generator will yield corresponding quantised matrices for each chunk of the TextFileReader object.\n",
    "    elif isinstance(data, TextFileReader):\n",
    "        return (chunk[text_col].apply(quant_single_text) for chunk in data)\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"\"\"Allowed types are one of the following: \n",
    "            - pandas.DataFrame\n",
    "            - Generator[pandas.DataFrame, None, None]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th></th>\n",
       "      <th>=</th>\n",
       "      <th>&lt;</th>\n",
       "      <th>&gt;</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>[</th>\n",
       "      <th>]</th>\n",
       "      <th>{</th>\n",
       "      <th>}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        a    b    c    d    e    f    g    h    i    j  ...         =    <  \\\n",
       "0     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "2     0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "4     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1010  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1011  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1012  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1013  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "        >    (    )    [    ]    {    }  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1010  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1011  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1012  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1013  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1014 rows x 69 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test single text input\n",
    "text = 'a b cðŸ˜œ'\n",
    "t = quant_text(text)\n",
    "text_df = t[t.any(axis=1)].transpose()\n",
    "characters = [text_df[text_df[col]==1].index[0] for col in text_df][::-1]\n",
    "text = \"\".join(characters)\n",
    "print(text)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike was delivered damage,bike was delivered today, it was damage in transit, i'm now working through the process with haulbikes to get it repaired. when loading the bike adjacent to mine in the trailer the other bike tipped over and made a large dent in the gas tank on my bike. i've started the process of reporting it to haulbikes, contacting customer service within 24 hours, sending photos, getting quotes and such. still unsure if i'm liable for the $500 deductible. in the end the paperwork states that i'm suppose to return the damaged items at my expense to haulbikes. guess i need to find out if i can actually legally ship a gas tank that has had gas in it? what a hassle!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th></th>\n",
       "      <th>=</th>\n",
       "      <th>&lt;</th>\n",
       "      <th>&gt;</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>[</th>\n",
       "      <th>]</th>\n",
       "      <th>{</th>\n",
       "      <th>}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        a    b    c    d    e    f    g    h    i    j  ...         =    <  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1010  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1011  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1012  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1013  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "        >    (    )    [    ]    {    }  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1010  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1011  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1012  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1013  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1014 rows x 69 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test series input\n",
    "test_df = read_csv('../data/test/test_clean.csv')\n",
    "t = quant_text(test_df['comment'].head()).iloc[0]\n",
    "# Reverse matrix into text to check\n",
    "text_df = t[t.any(axis=1)].transpose()\n",
    "characters = [text_df[text_df[col]==1].index[0] for col in text_df][::-1]\n",
    "text = \"\".join(characters)\n",
    "print(text)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike was delivered damage,bike was delivered today, it was damage in transit, i'm now working through the process with haulbikes to get it repaired. when loading the bike adjacent to mine in the trailer the other bike tipped over and made a large dent in the gas tank on my bike. i've started the process of reporting it to haulbikes, contacting customer service within 24 hours, sending photos, getting quotes and such. still unsure if i'm liable for the $500 deductible. in the end the paperwork states that i'm suppose to return the damaged items at my expense to haulbikes. guess i need to find out if i can actually legally ship a gas tank that has had gas in it? what a hassle!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th></th>\n",
       "      <th>=</th>\n",
       "      <th>&lt;</th>\n",
       "      <th>&gt;</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>[</th>\n",
       "      <th>]</th>\n",
       "      <th>{</th>\n",
       "      <th>}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        a    b    c    d    e    f    g    h    i    j  ...         =    <  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1010  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1011  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1012  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1013  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "        >    (    )    [    ]    {    }  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1010  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1011  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1012  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1013  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1014 rows x 69 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test TextFileReader input\n",
    "test_df = read_csv('../data/test/test_clean.csv', chunksize=5)\n",
    "t_generator = quant_text(test_df)\n",
    "t = next(t_generator).iloc[0]\n",
    "# Reverse matrix into text to check\n",
    "text_df = t[t.any(axis=1)].transpose()\n",
    "characters = [text_df[text_df[col]==1].index[0] for col in text_df][::-1]\n",
    "text = \"\".join(characters)\n",
    "print(text)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a custom Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually many attributes, options and states that exists with the data loading process and can benefit from an OOP structure. The class should take input of the actual dataset (cleaned texts and labels), and hold its states after applying the quantisation step with conversion to torch tensors.\n",
    "\n",
    "In fact, it will be better to include the quantisation function inside of this object as a class method since we can use this function to convert raw input data with the standard `__getitem__` function, and allow simple indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Generator\n",
    "from pandas import read_csv, DataFrame, Series, concat\n",
    "from pandas.io.parsers import TextFileReader\n",
    "from numpy import identity, zeros\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom class created for converting a raw text/label dataset into quantised\n",
    "    version of itself, and storing relevant textual attributes. This class subclass \n",
    "    torch.utils.data.Dataset with the __len__ method overriden to return the number \n",
    "    of text inputs, and the __getitem__ method overriden to return the quantised \n",
    "    version of the text data elements and their labels. The dimension of the returned \n",
    "    quantised text is directly related to the class attributes i.e number of characters\n",
    "    in the alphbet and maximum length of text input. These class attributes have default \n",
    "    values and may be altered by the user.\n",
    "    \n",
    "    *Note: the alphabet and max_length class attributes can be considered as hyperparameters \n",
    "    for the model training process. If these parameters are to be altered for model \n",
    "    performance experiments, the user may change them at a class level.\n",
    "    \n",
    "    :param alphabet: the alphabet to be used for text quantisation. It is critical to escape \n",
    "    special characters.\n",
    "    :type alphabet: str\n",
    "    :param max_length: the maximum text input length used for either truncating or padding \n",
    "    of the quantised text matrix.\n",
    "    :type max_length: int\n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\"\n",
    "    max_length = 1014\n",
    "    identity_mat = DataFrame(identity(len(alphabet)), dtype='float32', \n",
    "                             index=list(alphabet), columns=list(alphabet))\n",
    "    \n",
    "    def __init__(self, texts: Series, labels: Series):\n",
    "        \"\"\"\n",
    "        Class constructor taking texts and corresponding labels as inputs\n",
    "        :param texts: the raw text data\n",
    "        :type texts: Series\n",
    "        :param labels: the corresponding labels to the raw text data.\n",
    "        :type lables: Series\n",
    "        \"\"\"\n",
    "        self.raw_texts = texts\n",
    "        self.raw_labels = labels\n",
    "        self.length = texts.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Overiding the __getitem__ method. Return quantised version of an text\n",
    "        element together with its label. The intention of this function is to turn \n",
    "        the custom dataset into an \"map-style dataset\" which can be passed to a \n",
    "        torch.utils.data.DataLoader object to allow batch processing further down \n",
    "        the pipeline.\n",
    "        \"\"\"\n",
    "        raw_text = self.raw_texts.iloc[index]\n",
    "        label = self.raw_labels.iloc[index]\n",
    "        quantised_text = TextDataset.quant_text(raw_text).to_numpy()\n",
    "        return Tensor(quantised_text), label\n",
    "    \n",
    "    @classmethod\n",
    "    def quant_text(cls, data: Union[str, Series]) -> Union[DataFrame, Series]:\n",
    "        \"\"\"\n",
    "        Takes an input text set (can be a single text input or a series of text inputs), \n",
    "        and returns the quantised version of all its elements. This method may be called \n",
    "        at a class level, which will allow external data outside of the instance to be \n",
    "        pass as input to this method. This will be particularly usedful during inference \n",
    "        when text must be quantised first.\n",
    "\n",
    "        :param data: text input on which quantisation shall be performed.\n",
    "        :type data: Union[str, Series]\n",
    "        :return: quantised text.\n",
    "        :rtype: Union[DataFrame, Series[DataFrame]]\n",
    "        \"\"\"\n",
    "\n",
    "        def quant_single_text(data: str):\n",
    "            text_mat = None\n",
    "            # Catch scenarios where text is an empty string, do nothing in this case\n",
    "            if len(data) > 0:\n",
    "                # Character vectors need to be constructed in reverse order\n",
    "                # This is to feed data into the CNN in the right order.\n",
    "                reversed_text = data[::-1]\n",
    "                # Create quantised text matrix with:\n",
    "                # columns: characters in alphabet\n",
    "                # rows: characters in input string, fixed size to max_length\n",
    "                text_mat = concat([cls.identity_mat.loc[c] for c in reversed_text \n",
    "                                   if c in cls.alphabet], axis=1)\n",
    "                text_mat = text_mat.transpose().reset_index(drop=True)\n",
    "\n",
    "                # Truncate text matrix if input text length is more than max length\n",
    "                if text_mat.shape[0] > cls.max_length:\n",
    "                    text_mat = text_mat.iloc[:cls.max_length]\n",
    "\n",
    "                # Pad text matrix if input text length is less than max length\n",
    "                elif 0 < text_mat.shape[0] < cls.max_length:\n",
    "                    diff = cls.max_length - text_mat.shape[0]\n",
    "                    df_to_append = DataFrame(zeros((diff, len(cls.alphabet))), columns=list(cls.alphabet))\n",
    "                    text_mat = concat([text_mat, df_to_append], axis=0).reset_index(drop=True)\n",
    "            return text_mat\n",
    "\n",
    "        # If input is a single text, then output its corresponding quantised matrix\n",
    "        if isinstance(data, str):\n",
    "            return quant_single_text(data)\n",
    "\n",
    "        # If input is a series of texts, then output a series with corresponding quantised matrices\n",
    "        elif isinstance(data, Series):\n",
    "            return data.apply(quant_single_text)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"\"\"Allowed types are one of the following: \n",
    "                - str\n",
    "                - pandas.Series[str]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
